{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0437c2c",
   "metadata": {},
   "source": [
    "# What is this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a06db8",
   "metadata": {},
   "source": [
    "This preprocessing pipeline prepares the dataset for the hybrid fake review detection framework, combining:\n",
    "\n",
    "- SGDCTH’s group-based spammer detection\n",
    "- SL-GAD’s self-supervised graph representation learning (GNN encoder)\n",
    "\n",
    "It converts raw Amazon review data into a heterogeneous information sub-network (HISN) with rich attribute features and temporal, textual, and behavioral signals.\n",
    "The resulting dataset is ready for use in Stage 2: self-supervised GNN training and Stage 3: DBSCAN-based candidate group detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd22682",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccaf7d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from collections import Counter\n",
    "import emoji\n",
    "import re\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1570648e",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4be9a352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>images</th>\n",
       "      <th>asin</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>helpful_vote</th>\n",
       "      <th>verified_purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Pretty locket</td>\n",
       "      <td>I think this locket is really pretty. The insi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B00LOPVX74</td>\n",
       "      <td>B00LOPVX74</td>\n",
       "      <td>AGBFYI2DDIKXC5Y4FARTYDTQBMFQ</td>\n",
       "      <td>2020-01-09 00:06:34.489</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>Great</td>\n",
       "      <td>[]</td>\n",
       "      <td>B07B4JXK8D</td>\n",
       "      <td>B07B4JXK8D</td>\n",
       "      <td>AFQLNQNQYFWQZPJQZS6V3NZU4QBQ</td>\n",
       "      <td>2020-12-20 01:04:06.701</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>One of the stones fell out within the first 2 ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B007ZSEQ4Q</td>\n",
       "      <td>B007ZSEQ4Q</td>\n",
       "      <td>AHITBJSS7KYUBVZPX7M2WJCOIVKQ</td>\n",
       "      <td>2015-05-23 01:33:48.000</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Won’t buy again</td>\n",
       "      <td>Crappy socks. Money wasted. Bought to wear wit...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B07F2BTFS9</td>\n",
       "      <td>B07F2BTFS9</td>\n",
       "      <td>AFVNEEPDEIH5SPUN5BWC6NKL3WNQ</td>\n",
       "      <td>2018-12-31 20:57:27.095</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I LOVE these glasses</td>\n",
       "      <td>I LOVE these glasses!  They fit perfectly over...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B00PKRFU4O</td>\n",
       "      <td>B00XESJTDE</td>\n",
       "      <td>AHSPLDNW5OOUK2PLH7GXLACFBZNQ</td>\n",
       "      <td>2015-08-13 14:29:26.000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                 title  \\\n",
       "0       5         Pretty locket   \n",
       "1       5                     A   \n",
       "2       2             Two Stars   \n",
       "3       1       Won’t buy again   \n",
       "4       5  I LOVE these glasses   \n",
       "\n",
       "                                                text images        asin  \\\n",
       "0  I think this locket is really pretty. The insi...     []  B00LOPVX74   \n",
       "1                                              Great     []  B07B4JXK8D   \n",
       "2  One of the stones fell out within the first 2 ...     []  B007ZSEQ4Q   \n",
       "3  Crappy socks. Money wasted. Bought to wear wit...     []  B07F2BTFS9   \n",
       "4  I LOVE these glasses!  They fit perfectly over...     []  B00PKRFU4O   \n",
       "\n",
       "  parent_asin                       user_id               timestamp  \\\n",
       "0  B00LOPVX74  AGBFYI2DDIKXC5Y4FARTYDTQBMFQ 2020-01-09 00:06:34.489   \n",
       "1  B07B4JXK8D  AFQLNQNQYFWQZPJQZS6V3NZU4QBQ 2020-12-20 01:04:06.701   \n",
       "2  B007ZSEQ4Q  AHITBJSS7KYUBVZPX7M2WJCOIVKQ 2015-05-23 01:33:48.000   \n",
       "3  B07F2BTFS9  AFVNEEPDEIH5SPUN5BWC6NKL3WNQ 2018-12-31 20:57:27.095   \n",
       "4  B00XESJTDE  AHSPLDNW5OOUK2PLH7GXLACFBZNQ 2015-08-13 14:29:26.000   \n",
       "\n",
       "   helpful_vote  verified_purchase  \n",
       "0             3               True  \n",
       "1             0               True  \n",
       "2             3               True  \n",
       "3             2               True  \n",
       "4             0               True  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"Amazon_Fashion.jsonl\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fff3757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2500939 entries, 0 to 2500938\n",
      "Data columns (total 10 columns):\n",
      " #   Column             Dtype         \n",
      "---  ------             -----         \n",
      " 0   rating             int64         \n",
      " 1   title              object        \n",
      " 2   text               object        \n",
      " 3   images             object        \n",
      " 4   asin               object        \n",
      " 5   parent_asin        object        \n",
      " 6   user_id            object        \n",
      " 7   timestamp          datetime64[ns]\n",
      " 8   helpful_vote       int64         \n",
      " 9   verified_purchase  bool          \n",
      "dtypes: bool(1), datetime64[ns](1), int64(2), object(6)\n",
      "memory usage: 174.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792563ab",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d9c58a",
   "metadata": {},
   "source": [
    "## K-Core Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe645787",
   "metadata": {},
   "source": [
    "K-Core filtering is needed to done in review dataset with some purpose:\n",
    "- To reduce sparsity of the data, because there are much data that only have 1 reviews or reviewer that only have reviewed 1 product\n",
    "- Fairer evaluation, especially if using GNN. History or neighbor is needed to make our model learn\n",
    "- Stability and reproducibility for the data\n",
    "- Computationally more efficient since we have less data\n",
    "\n",
    "As for this data, we will perform a K-Core filtering with 3-2-Core rule where \n",
    "- Each item need to have 3 reviews each, and\n",
    "- Each user need to have 2 reviews each\n",
    "\n",
    "This is done incrementally until the requirements met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "258494b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: 2500939 rows\n",
      "After item filtering: 1679162 rows\n",
      "After user filtering: 398409 rows\n",
      "Filtered 2102530 rows in this iteration.\n",
      "\n",
      "Iteration 2: 398409 rows\n",
      "After item filtering: 266837 rows\n",
      "After user filtering: 211865 rows\n",
      "Filtered 186544 rows in this iteration.\n",
      "\n",
      "Iteration 3: 211865 rows\n",
      "After item filtering: 192192 rows\n",
      "After user filtering: 180792 rows\n",
      "Filtered 31073 rows in this iteration.\n",
      "\n",
      "Iteration 4: 180792 rows\n",
      "After item filtering: 175586 rows\n",
      "After user filtering: 172310 rows\n",
      "Filtered 8482 rows in this iteration.\n",
      "\n",
      "Iteration 5: 172310 rows\n",
      "After item filtering: 170819 rows\n",
      "After user filtering: 169845 rows\n",
      "Filtered 2465 rows in this iteration.\n",
      "\n",
      "Iteration 6: 169845 rows\n",
      "After item filtering: 169384 rows\n",
      "After user filtering: 169091 rows\n",
      "Filtered 754 rows in this iteration.\n",
      "\n",
      "Iteration 7: 169091 rows\n",
      "After item filtering: 168945 rows\n",
      "After user filtering: 168848 rows\n",
      "Filtered 243 rows in this iteration.\n",
      "\n",
      "Iteration 8: 168848 rows\n",
      "After item filtering: 168802 rows\n",
      "After user filtering: 168770 rows\n",
      "Filtered 78 rows in this iteration.\n",
      "\n",
      "Iteration 9: 168770 rows\n",
      "After item filtering: 168750 rows\n",
      "After user filtering: 168738 rows\n",
      "Filtered 32 rows in this iteration.\n",
      "\n",
      "Iteration 10: 168738 rows\n",
      "After item filtering: 168732 rows\n",
      "After user filtering: 168728 rows\n",
      "Filtered 10 rows in this iteration.\n",
      "\n",
      "Iteration 11: 168728 rows\n",
      "After item filtering: 168728 rows\n",
      "After user filtering: 168728 rows\n",
      "Filtered 0 rows in this iteration.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def k_core_filtering(df, k_items = 3, k_users = 2):\n",
    "    iteration = 0\n",
    "    \n",
    "    while True:\n",
    "        iteration += 1\n",
    "        print(f\"Iteration {iteration}: {len(df)} rows\")\n",
    "        \n",
    "        before_len = len(df)\n",
    "        \n",
    "        # First we filter by the item count that atleast there is 3 review\n",
    "        item_counts = df['asin'].value_counts()\n",
    "        active_items = item_counts[item_counts >= k_items].index\n",
    "        df = df[df['asin'].isin(active_items)]\n",
    "        \n",
    "        print(f\"After item filtering: {len(df)} rows\")\n",
    "        \n",
    "        # Then we filter by the user count that atleast there is 3 review\n",
    "        user_counts = df['user_id'].value_counts()\n",
    "        active_users = user_counts[user_counts >= k_users].index\n",
    "        df = df[df['user_id'].isin(active_users)]\n",
    "        \n",
    "        print(f\"After user filtering: {len(df)} rows\")\n",
    "        \n",
    "        after_len = len(df)\n",
    "        \n",
    "        print(f\"Filtered {before_len - after_len} rows in this iteration.\\n\")\n",
    "        \n",
    "        if before_len == after_len:\n",
    "            break\n",
    "        \n",
    "    return df\n",
    "\n",
    "data = k_core_filtering(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87cf9e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users in the core filtering: 69587\n",
      "Number of unique items in the core filtering: 29282\n",
      "Total reviews (edges) in the core filtering: 168728\n",
      "Average reviews per user: 2.42\n",
      "Average reviews per item: 5.76\n"
     ]
    }
   ],
   "source": [
    "num_users = data['user_id'].nunique()\n",
    "num_items = data['asin'].nunique()\n",
    "num_reviews = len(data)\n",
    "\n",
    "print(f\"Number of unique users in the core filtering: {num_users}\")\n",
    "print(f\"Number of unique items in the core filtering: {num_items}\")\n",
    "print(f\"Total reviews (edges) in the core filtering: {num_reviews}\")\n",
    "\n",
    "if num_users > 0:\n",
    "    print(f\"Average reviews per user: {num_reviews / num_users:.2f}\")\n",
    "if num_items > 0:\n",
    "    print(f\"Average reviews per item: {num_reviews / num_items:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7482d2",
   "metadata": {},
   "source": [
    "After the K-Core filtering, we end up with 168.728 data which means we are left with less than 10% of the data. This shows in the raw data there are too many noise and sparsity that we dont need to train our model and create a strong fake review detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7c2ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data_kcore_3_2.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2201be58",
   "metadata": {},
   "source": [
    "## Basic data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58666a",
   "metadata": {},
   "source": [
    "To make the next processess to be done smoothly, some of the data especially the texts data where it contains title and review, need to be cleaned and normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59c488fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the timestanp to be datetime type data\n",
    "data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# Combine title + text\n",
    "data[\"full_text\"] = (data[\"title\"] + \" \" + data[\"text\"]).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ea614de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" <URL> \", text)\n",
    "\n",
    "    # 3. Remove emails\n",
    "    text = re.sub(r\"\\S+@\\S+\", \" <EMAIL> \", text)\n",
    "\n",
    "    # 4. Normalize repeated characters (e.g. coooool -> coool)\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n",
    "\n",
    "    # 5. Remove weird symbols (keep normal punctuation)\n",
    "    text = re.sub(r\"[^a-z0-9\\s\\.,!?']\", \" \", text)\n",
    "\n",
    "    # 6. Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "data[\"full_text\"] = data[\"full_text\"].apply(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55c2d138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>images</th>\n",
       "      <th>asin</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>helpful_vote</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5</td>\n",
       "      <td>Great socks for the gym or for your house</td>\n",
       "      <td>Nice socks. They say they are for yoga/pilates...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B0856TH4LK</td>\n",
       "      <td>B0856TH4LK</td>\n",
       "      <td>AFSKPY37N3C43SOI5IEXEK5JSIYA</td>\n",
       "      <td>2020-04-18 10:24:18.621</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>great socks for the gym or for your house nice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>Nice, warm merino wool socks!</td>\n",
       "      <td>I absolutely love these socks. Super soft and ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B07ZWZ2595</td>\n",
       "      <td>B07ZWZ2595</td>\n",
       "      <td>AFSKPY37N3C43SOI5IEXEK5JSIYA</td>\n",
       "      <td>2020-01-19 11:23:39.201</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>nice, warm merino wool socks! i absolutely lov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>Nice jacket, runs a little small</td>\n",
       "      <td>Not a bad rain jacket but, at the time I am wr...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B07NY72H7W</td>\n",
       "      <td>B07NY72H7W</td>\n",
       "      <td>AFSKPY37N3C43SOI5IEXEK5JSIYA</td>\n",
       "      <td>2019-08-12 19:43:59.659</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>nice jacket, runs a little small not a bad rai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5</td>\n",
       "      <td>Great biking gloves for newbies or expert riders</td>\n",
       "      <td>Great cycling gloves. Well made, form fitting ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B07NX5RHZ2</td>\n",
       "      <td>B07NX5RHZ2</td>\n",
       "      <td>AFSKPY37N3C43SOI5IEXEK5JSIYA</td>\n",
       "      <td>2019-06-24 01:00:44.117</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>great biking gloves for newbies or expert ride...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>Quality made but runs small</td>\n",
       "      <td>Nicely made, lightweight windbreaker. Easily f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B07H92ZCQ9</td>\n",
       "      <td>B07H92ZCQ9</td>\n",
       "      <td>AFSKPY37N3C43SOI5IEXEK5JSIYA</td>\n",
       "      <td>2019-02-09 11:46:44.435</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>quality made but runs small nicely made, light...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rating                                             title  \\\n",
       "28       5         Great socks for the gym or for your house   \n",
       "29       5                     Nice, warm merino wool socks!   \n",
       "30       4                  Nice jacket, runs a little small   \n",
       "31       5  Great biking gloves for newbies or expert riders   \n",
       "32       3                       Quality made but runs small   \n",
       "\n",
       "                                                 text images        asin  \\\n",
       "28  Nice socks. They say they are for yoga/pilates...     []  B0856TH4LK   \n",
       "29  I absolutely love these socks. Super soft and ...     []  B07ZWZ2595   \n",
       "30  Not a bad rain jacket but, at the time I am wr...     []  B07NY72H7W   \n",
       "31  Great cycling gloves. Well made, form fitting ...     []  B07NX5RHZ2   \n",
       "32  Nicely made, lightweight windbreaker. Easily f...     []  B07H92ZCQ9   \n",
       "\n",
       "   parent_asin                       user_id               timestamp  \\\n",
       "28  B0856TH4LK  AFSKPY37N3C43SOI5IEXEK5JSIYA 2020-04-18 10:24:18.621   \n",
       "29  B07ZWZ2595  AFSKPY37N3C43SOI5IEXEK5JSIYA 2020-01-19 11:23:39.201   \n",
       "30  B07NY72H7W  AFSKPY37N3C43SOI5IEXEK5JSIYA 2019-08-12 19:43:59.659   \n",
       "31  B07NX5RHZ2  AFSKPY37N3C43SOI5IEXEK5JSIYA 2019-06-24 01:00:44.117   \n",
       "32  B07H92ZCQ9  AFSKPY37N3C43SOI5IEXEK5JSIYA 2019-02-09 11:46:44.435   \n",
       "\n",
       "    helpful_vote  verified_purchase  \\\n",
       "28             0              False   \n",
       "29             0              False   \n",
       "30             0              False   \n",
       "31             0              False   \n",
       "32             0              False   \n",
       "\n",
       "                                            full_text  \n",
       "28  great socks for the gym or for your house nice...  \n",
       "29  nice, warm merino wool socks! i absolutely lov...  \n",
       "30  nice jacket, runs a little small not a bad rai...  \n",
       "31  great biking gloves for newbies or expert ride...  \n",
       "32  quality made but runs small nicely made, light...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2227e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=\"parent_asin\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68cef9f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>images</th>\n",
       "      <th>asin</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>helpful_vote</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5</td>\n",
       "      <td>Great socks for the gym or for your house</td>\n",
       "      <td>Nice socks. They say they are for yoga/pilates...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B0856TH4LK</td>\n",
       "      <td>AFSKPY37N3C43SOI5IEXEK5JSIYA</td>\n",
       "      <td>2020-04-18 10:24:18.621</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>great socks for the gym or for your house nice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>Nice, warm merino wool socks!</td>\n",
       "      <td>I absolutely love these socks. Super soft and ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B07ZWZ2595</td>\n",
       "      <td>AFSKPY37N3C43SOI5IEXEK5JSIYA</td>\n",
       "      <td>2020-01-19 11:23:39.201</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>nice, warm merino wool socks! i absolutely lov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>Nice jacket, runs a little small</td>\n",
       "      <td>Not a bad rain jacket but, at the time I am wr...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B07NY72H7W</td>\n",
       "      <td>AFSKPY37N3C43SOI5IEXEK5JSIYA</td>\n",
       "      <td>2019-08-12 19:43:59.659</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>nice jacket, runs a little small not a bad rai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5</td>\n",
       "      <td>Great biking gloves for newbies or expert riders</td>\n",
       "      <td>Great cycling gloves. Well made, form fitting ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B07NX5RHZ2</td>\n",
       "      <td>AFSKPY37N3C43SOI5IEXEK5JSIYA</td>\n",
       "      <td>2019-06-24 01:00:44.117</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>great biking gloves for newbies or expert ride...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>Quality made but runs small</td>\n",
       "      <td>Nicely made, lightweight windbreaker. Easily f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B07H92ZCQ9</td>\n",
       "      <td>AFSKPY37N3C43SOI5IEXEK5JSIYA</td>\n",
       "      <td>2019-02-09 11:46:44.435</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>quality made but runs small nicely made, light...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rating                                             title  \\\n",
       "28       5         Great socks for the gym or for your house   \n",
       "29       5                     Nice, warm merino wool socks!   \n",
       "30       4                  Nice jacket, runs a little small   \n",
       "31       5  Great biking gloves for newbies or expert riders   \n",
       "32       3                       Quality made but runs small   \n",
       "\n",
       "                                                 text images        asin  \\\n",
       "28  Nice socks. They say they are for yoga/pilates...     []  B0856TH4LK   \n",
       "29  I absolutely love these socks. Super soft and ...     []  B07ZWZ2595   \n",
       "30  Not a bad rain jacket but, at the time I am wr...     []  B07NY72H7W   \n",
       "31  Great cycling gloves. Well made, form fitting ...     []  B07NX5RHZ2   \n",
       "32  Nicely made, lightweight windbreaker. Easily f...     []  B07H92ZCQ9   \n",
       "\n",
       "                         user_id               timestamp  helpful_vote  \\\n",
       "28  AFSKPY37N3C43SOI5IEXEK5JSIYA 2020-04-18 10:24:18.621             0   \n",
       "29  AFSKPY37N3C43SOI5IEXEK5JSIYA 2020-01-19 11:23:39.201             0   \n",
       "30  AFSKPY37N3C43SOI5IEXEK5JSIYA 2019-08-12 19:43:59.659             0   \n",
       "31  AFSKPY37N3C43SOI5IEXEK5JSIYA 2019-06-24 01:00:44.117             0   \n",
       "32  AFSKPY37N3C43SOI5IEXEK5JSIYA 2019-02-09 11:46:44.435             0   \n",
       "\n",
       "    verified_purchase                                          full_text  \n",
       "28              False  great socks for the gym or for your house nice...  \n",
       "29              False  nice, warm merino wool socks! i absolutely lov...  \n",
       "30              False  nice jacket, runs a little small not a bad rai...  \n",
       "31              False  great biking gloves for newbies or expert ride...  \n",
       "32              False  quality made but runs small nicely made, light...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e75981",
   "metadata": {},
   "source": [
    "## Text Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d0d91d",
   "metadata": {},
   "source": [
    "> Purpose:\n",
    "\n",
    "Generate 768-dimensional dense text embeddings representing the semantic meaning of each review.\n",
    "\n",
    "> Why this model:\n",
    "\n",
    "Multilingual support (Amazon data spans multiple locales)\n",
    "\n",
    "Strong semantic representation suitable for similarity and anomaly analysis.\n",
    "\n",
    "> Output:\n",
    "\n",
    "text_embs.shape = (num_reviews, 768)\n",
    "→ used as review attribute features in the HISN graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7887b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55af1dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17795d897dfd4f6a962cff5193c37fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL = \"sentence-transformers/paraphrase-xlm-r-multilingual-v1\"\n",
    "embedding_model = SentenceTransformer(MODEL)\n",
    "\n",
    "data = data.reset_index(drop=True)\n",
    "text_embs = embedding_model.encode(data[\"full_text\"].tolist(), batch_size=512, show_progress_bar=True)\n",
    "text_embs = np.array(text_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7a08bb",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d6bb68",
   "metadata": {},
   "source": [
    "### Basic numerical\n",
    "- review length : number of words in review text --> detect if there is unnatural review patterns\n",
    "- dup count = number of identical texts --> Measure if there is duplication in the review\n",
    "- is_duplicate = Binary flag for duplicate reviews --> detect text reuse or copy-paste\n",
    "- helpful_vote -> log_helpful = log transformed helpful votes --> smooth heavy tailed distribution\n",
    "- verified purchsse -> integered --> normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e35d1742",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"review_length\"] = data[\"text\"].apply(lambda x: len(x.split()))\n",
    "data[\"dup_count\"] = data[\"text\"].map(data[\"text\"].value_counts())   # how many times this text appears\n",
    "data[\"is_duplicate\"] = (data[\"dup_count\"] > 1).astype(int)\n",
    "data[\"log_helpful\"] = np.log1p(data[\"helpful_vote\"])\n",
    "data[\"verified_int\"] = data[\"verified_purchase\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5f75bc",
   "metadata": {},
   "source": [
    "### Temporal Cyclic Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d113a18",
   "metadata": {},
   "source": [
    "In this block we add time of day and day of year as continous cyclic features, preservice periodic patterns. \n",
    "- Reviews may exhibit temporal bursts typical of spam campaigns\n",
    "- Cyclic encoding captures daily/seasonal rhythms better than raw timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "179192c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"day\"] = data[\"timestamp\"].dt.dayofyear\n",
    "data[\"hour\"] = data[\"timestamp\"].dt.hour\n",
    "data[\"sin_day\"] = np.sin(2*np.pi*data[\"day\"]/365)\n",
    "data[\"cos_day\"] = np.cos(2*np.pi*data[\"day\"]/365)\n",
    "data[\"sin_hour\"] = np.sin(2*np.pi*data[\"hour\"]/24)\n",
    "data[\"cos_hour\"] = np.cos(2*np.pi*data[\"hour\"]/24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62985cb7",
   "metadata": {},
   "source": [
    "## Making User and Items Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e43fb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stats = data.groupby(\"user_id\").agg(\n",
    "    n_reviews_user=(\"rating\",\"count\"),\n",
    "    avg_rating_user=(\"rating\",\"mean\"),\n",
    "    std_rating_user=(\"rating\",\"std\"),\n",
    "    frac_verified_user=(\"verified_int\",\"mean\"),\n",
    "    avg_len_user=(\"review_length\",\"mean\"),\n",
    "    dup_ratio_user=(\"is_duplicate\",\"mean\")\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d92fa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_stats = data.groupby(\"asin\").agg(\n",
    "    n_reviews_item=(\"rating\",\"count\"),\n",
    "    avg_rating_item=(\"rating\",\"mean\"),\n",
    "    std_rating_item=(\"rating\",\"std\"),\n",
    "    dup_ratio_item=(\"is_duplicate\",\"mean\")\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73c8a22",
   "metadata": {},
   "source": [
    "### User features engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43af2d5",
   "metadata": {},
   "source": [
    "This one is to detect the burst of reviews in a day, like hoew big is it and how consistent it happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c41e5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# 1. Reviews per day statistics\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Reviews per user per day\n",
    "data['date'] = data['timestamp'].dt.date\n",
    "user_daily = data.groupby(['user_id', 'date']).size().reset_index(name='reviews_per_day')\n",
    "\n",
    "# Max reviews in a single day per user\n",
    "user_burst = user_daily.groupby('user_id')['reviews_per_day'].max().reset_index()\n",
    "user_burst.rename(columns={'reviews_per_day': 'max_reviews_per_day'}, inplace=True)\n",
    "\n",
    "# Entropy of review days per user (spread vs bursty)\n",
    "def entropy(arr):\n",
    "    probs = arr / arr.sum()\n",
    "    return -np.sum(probs * np.log(probs + 1e-10))\n",
    "\n",
    "user_entropy = user_daily.groupby('user_id')['reviews_per_day'].apply(entropy).reset_index()\n",
    "user_entropy.rename(columns={'reviews_per_day': 'day_entropy'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9954a2",
   "metadata": {},
   "source": [
    "This one is to detect how similasr is a review of users to their other reviews to detect if there are any copy-pasted reviews in different products.\n",
    "\n",
    "This is critical because In SL-GAD, the contrastive module learns consistency between views of the same node.<br>\n",
    "If a user already has low diversity (high text similarity), their embeddings will appear artificially stable —<br>\n",
    "→ the model must learn to disentangle “stable because spammy” vs. “stable because natural”.\n",
    "These similarity features help the GNN capture that nuance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f455a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# 2. Similarity Features (review text duplicates/similarity)\n",
    "# -------------------------------------------------\n",
    "similarities = []\n",
    "for uid, group in data.groupby('user_id'):\n",
    "    if len(group) > 1:\n",
    "        embs = text_embs[group.index]\n",
    "        sim_matrix = cosine_similarity(embs)\n",
    "        # Upper triangle mean (excluding self)\n",
    "        triu = sim_matrix[np.triu_indices_from(sim_matrix, k=1)]\n",
    "        avg_sim = triu.mean() if len(triu) > 0 else 0\n",
    "    else:\n",
    "        avg_sim = 0\n",
    "    similarities.append((uid, avg_sim))\n",
    "    \n",
    "user_sim = pd.DataFrame(similarities, columns=['user_id','avg_text_sim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92714998",
   "metadata": {},
   "source": [
    "Its to detect the network of the users and the items based on degree centrality\n",
    "1. Users (n_unique_items)\n",
    "- Meaning : Number of distinct items a user reviewed \n",
    "- Intuition : Diversity of reviewed items<br>\n",
    "Low : Focused reviewer (could be genuine niche interest)<br>\n",
    "High : possible spammer reviewing too many unrelated products\n",
    "\n",
    "2. Items (n_unique_users)\n",
    "- Meaning : Number of distinct users reviewing an item\n",
    "- Intuition : Item popularity<br>\n",
    "Very high → popular item (normal).<br>\n",
    "Very low → suspicious item possibly targeted by fake reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "190fe5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# 3. Network-based Features\n",
    "# -------------------------------------------------\n",
    "# Degree centrality\n",
    "user_degree = data.groupby('user_id')['asin'].nunique().reset_index()\n",
    "user_degree.rename(columns={'asin': 'n_unique_items'}, inplace=True)\n",
    "\n",
    "item_degree = data.groupby('asin')['user_id'].nunique().reset_index()\n",
    "item_degree.rename(columns={'user_id': 'n_unique_users'}, inplace=True)\n",
    "\n",
    "# Density in user-item neighborhood: reviews / (user_degree * item_degree)\n",
    "data['user_degree'] = data['user_id'].map(user_degree.set_index('user_id')['n_unique_items'])\n",
    "data['item_degree'] = data['asin'].map(item_degree.set_index('asin')['n_unique_users'])\n",
    "data['local_density'] = data['dup_count'] / (data['user_degree'] * data['item_degree'] + 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49de8183",
   "metadata": {},
   "source": [
    "### Stats and features merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e51edc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_7688\\1181698331.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  user_stats['avg_text_sim'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Userstats\n",
    "user_stats = user_stats.merge(user_burst, on='user_id', how='left')\n",
    "user_stats = user_stats.merge(user_entropy, on='user_id', how='left')\n",
    "user_stats = user_stats.merge(user_sim, on='user_id', how='left')\n",
    "user_stats = user_stats.merge(user_degree, on='user_id', how='left')\n",
    "\n",
    "user_stats['avg_text_sim'].fillna(0, inplace=True)\n",
    "user_stats.fillna(0, inplace=True)\n",
    "\n",
    "# Itemstats\n",
    "item_stats = item_stats.merge(item_degree, on='asin', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b72acf",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e759f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "columns_to_be_scaled = [\"rating\",\"review_length\",\"log_helpful\",\"verified_int\",\"dup_count\"]\n",
    "data_scaled = data[columns_to_be_scaled].copy()\n",
    "data_scaled = pd.DataFrame(scaler.fit_transform(data_scaled), columns=data_scaled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe12a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data_scaled.columns:\n",
    "    data[col+\"_scaled\"] = data_scaled[col]\n",
    "\n",
    "data.drop(columns=columns_to_be_scaled, inplace=True)\n",
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88612d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                   0\n",
       "text                    0\n",
       "images                  0\n",
       "asin                    0\n",
       "user_id                 0\n",
       "timestamp               0\n",
       "helpful_vote            0\n",
       "verified_purchase       0\n",
       "full_text               0\n",
       "is_duplicate            0\n",
       "day                     0\n",
       "hour                    0\n",
       "sin_day                 0\n",
       "cos_day                 0\n",
       "sin_hour                0\n",
       "cos_hour                0\n",
       "date                    0\n",
       "user_degree             0\n",
       "item_degree             0\n",
       "local_density           0\n",
       "rating_scaled           0\n",
       "review_length_scaled    0\n",
       "log_helpful_scaled      0\n",
       "verified_int_scaled     0\n",
       "dup_count_scaled        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf7477fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_features = np.hstack([\n",
    "    text_embs,\n",
    "    data[[\"rating_scaled\",\"review_length_scaled\",\"log_helpful_scaled\",\n",
    "        \"verified_int_scaled\",\"sin_day\",\"cos_day\",\"sin_hour\",\"cos_hour\",\"dup_count_scaled\"]].values\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78060ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review feature matrix shape: (168728, 777)\n",
      "User stats shape: (69587, 11)\n",
      "Item stats shape: (29282, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Review feature matrix shape:\", review_features.shape)\n",
    "print(\"User stats shape:\", user_stats.shape)\n",
    "print(\"Item stats shape:\", item_stats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97dfa04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final modeling-ready dataset shape: (168728, 15)\n"
     ]
    }
   ],
   "source": [
    "cols_to_keep = [\n",
    "    \"user_id\", \"asin\", \"timestamp\",\n",
    "    \"rating_scaled\", \"review_length_scaled\", \"log_helpful_scaled\",\n",
    "    \"verified_int_scaled\", \"dup_count_scaled\",\n",
    "    \"sin_day\", \"cos_day\", \"sin_hour\", \"cos_hour\",\n",
    "    \"user_degree\", \"item_degree\", \"local_density\"\n",
    "]\n",
    "\n",
    "data = data[cols_to_keep]\n",
    "print(\"✅ Final modeling-ready dataset shape:\", data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b96baf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All processed data saved.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "np.save(\"nb2/review_features.npy\", review_features)\n",
    "\n",
    "# Save user and item stats\n",
    "user_stats.to_csv(\"nb2/user_stats.csv\", index=False)\n",
    "item_stats.to_csv(\"nb2/item_stats.csv\", index=False)\n",
    "\n",
    "# Save processed DataFrame (with scaled columns)\n",
    "data.to_parquet(\"nb2/reviews_processed.parquet\", index=False)\n",
    "\n",
    "# Save ID mappings (for graph construction later)\n",
    "user2id = {u: i for i, u in enumerate(user_stats[\"user_id\"].unique())}\n",
    "item2id = {a: i for i, a in enumerate(item_stats[\"asin\"].unique())}\n",
    "review2id = {r: i for i, r in enumerate(data.index)}\n",
    "\n",
    "with open(\"nb2/id_mappings.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"user2id\": user2id, \"item2id\": item2id, \"review2id\": review2id}, f)\n",
    "\n",
    "print(\"✅ All processed data saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e430a6",
   "metadata": {},
   "source": [
    "# iseng nfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e8470b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"nb2/reviews_processed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8035e870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ NFS metrics calculated successfully!\n",
      "            asin  NRP       NDP  NTP     NFS_pre\n",
      "63    B000GAWSDG  367 -0.089483    4  147.973155\n",
      "57    B000FIS5U4  322 -0.093024    4  129.972093\n",
      "8810  B017U1FDM6  239  0.148553    3   96.544566\n",
      "6622  B00ZIK4NH8  195  0.102977    3   78.930893\n",
      "3500  B00KA3TUNA  180  0.532277    3   73.059683\n",
      "3507  B00KA3VX62  178  0.315525    3   72.194657\n",
      "5624  B00UDF11O6  161  0.204466    2   65.061340\n",
      "3504  B00KA3VEG6  157  0.154181    3   63.746254\n",
      "3499  B00KA3SRVG  156  0.120379    3   63.336114\n",
      "5308  B00SH9BD0W  147  0.089508    4   60.026852\n"
     ]
    }
   ],
   "source": [
    "# 1️⃣ NRP\n",
    "nrp = data.groupby('asin')['user_id'].nunique().reset_index()\n",
    "nrp.columns = ['asin', 'NRP']\n",
    "\n",
    "# 2️⃣ NDP (using dup_count_scaled)\n",
    "ndp = data.groupby('asin')['dup_count_scaled'].mean().reset_index()\n",
    "ndp.columns = ['asin', 'NDP']\n",
    "\n",
    "# 3️⃣ NTP\n",
    "data['date'] = data['timestamp'].dt.date\n",
    "burst_window = data.groupby(['asin', 'date']).size().reset_index(name='reviews_per_day')\n",
    "ntp = burst_window.groupby('asin')['reviews_per_day'].max().reset_index()\n",
    "ntp.columns = ['asin', 'NTP']\n",
    "\n",
    "# 4️⃣ Combine\n",
    "nfs = nrp.merge(ndp, on='asin', how='left').merge(ntp, on='asin', how='left')\n",
    "nfs.fillna(0, inplace=True)\n",
    "\n",
    "# Weighted combination\n",
    "alpha, beta, gamma = 0.4, 0.3, 0.3\n",
    "nfs['NFS_pre'] = alpha * nfs['NRP'] + beta * nfs['NDP'] + gamma * nfs['NTP']\n",
    "\n",
    "print(\"✅ NFS metrics calculated successfully!\")\n",
    "print(nfs.sort_values('NFS_pre', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b167e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = int(0.05 * len(data))    # or top 5%, depending on dataset\n",
    "target_asins = nfs.sort_values('NFS_pre', ascending=False).head(top_n)['asin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fd6e410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HISN base dataset: (94043, 16)\n"
     ]
    }
   ],
   "source": [
    "target_data = data[data['asin'].isin(target_asins)].copy()\n",
    "print(\"HISN base dataset:\", target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e3a7b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168728, 16)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff72114d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.74% of reviews belong to target products\n",
      "8436 target products out of 29282 total\n"
     ]
    }
   ],
   "source": [
    "coverage = len(target_data) / len(data)\n",
    "print(f\"{coverage*100:.2f}% of reviews belong to target products\")\n",
    "print(f\"{len(target_asins)} target products out of {data['asin'].nunique()} total\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da761dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 53418\n",
      "Items: 8436\n",
      "Reviews (edges): 94043\n",
      "Avg reviews per user: 1.76\n",
      "Avg reviews per item: 11.15\n"
     ]
    }
   ],
   "source": [
    "print(\"Users:\", target_data['user_id'].nunique())\n",
    "print(\"Items:\", target_data['asin'].nunique())\n",
    "print(\"Reviews (edges):\", len(target_data))\n",
    "\n",
    "avg_reviews_per_user = len(target_data) / target_data['user_id'].nunique()\n",
    "avg_reviews_per_item = len(target_data) / target_data['asin'].nunique()\n",
    "\n",
    "print(f\"Avg reviews per user: {avg_reviews_per_user:.2f}\")\n",
    "print(f\"Avg reviews per item: {avg_reviews_per_item:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23df6e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering:\n",
      "Users: 53418\n",
      "Items: 8436\n",
      "Reviews: 94043\n",
      "\n",
      "After removing 1-review users:\n",
      "Users: 29725\n",
      "Items: 8430\n",
      "Reviews: 70350\n",
      "\n",
      "After expanding to all reviews of active users:\n",
      "Users: 29725\n",
      "Items: 15881\n",
      "Reviews: 81922\n",
      "\n",
      "Avg reviews per user: 2.76\n",
      "Avg reviews per item: 5.16\n",
      "\n",
      "✅ Final HISN saved as 'hisn_final.parquet'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1️⃣  Start from your current target_data (after NFS filtering)\n",
    "# ============================================================\n",
    "print(\"Before filtering:\")\n",
    "print(\"Users:\", target_data['user_id'].nunique())\n",
    "print(\"Items:\", target_data['asin'].nunique())\n",
    "print(\"Reviews:\", len(target_data))\n",
    "\n",
    "# ============================================================\n",
    "# 2️⃣  Filter out one-shot users (those with only 1 review)\n",
    "# ============================================================\n",
    "user_counts = target_data['user_id'].value_counts()\n",
    "active_users = user_counts[user_counts >= 2].index\n",
    "\n",
    "filtered_data = target_data[target_data['user_id'].isin(active_users)].copy()\n",
    "\n",
    "print(\"\\nAfter removing 1-review users:\")\n",
    "print(\"Users:\", filtered_data['user_id'].nunique())\n",
    "print(\"Items:\", filtered_data['asin'].nunique())\n",
    "print(\"Reviews:\", len(filtered_data))\n",
    "\n",
    "# ============================================================\n",
    "# 3️⃣  Expand HISN neighborhood — include ALL reviews\n",
    "#      from these remaining active users (even for non-target items)\n",
    "# ============================================================\n",
    "target_users = filtered_data['user_id'].unique()\n",
    "expanded_data = data[data['user_id'].isin(target_users)].copy()\n",
    "\n",
    "print(\"\\nAfter expanding to all reviews of active users:\")\n",
    "print(\"Users:\", expanded_data['user_id'].nunique())\n",
    "print(\"Items:\", expanded_data['asin'].nunique())\n",
    "print(\"Reviews:\", len(expanded_data))\n",
    "\n",
    "# ============================================================\n",
    "# 4️⃣  Optional — verify average connectivity\n",
    "# ============================================================\n",
    "avg_reviews_per_user = len(expanded_data) / expanded_data['user_id'].nunique()\n",
    "avg_reviews_per_item = len(expanded_data) / expanded_data['asin'].nunique()\n",
    "\n",
    "print(f\"\\nAvg reviews per user: {avg_reviews_per_user:.2f}\")\n",
    "print(f\"Avg reviews per item: {avg_reviews_per_item:.2f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 5️⃣  Save this final HISN dataset for Stage 2 (SL-GAD training)\n",
    "# ============================================================\n",
    "expanded_data.to_parquet(\"nb2/hisn_final.parquet\", index=False)\n",
    "print(\"\\n✅ Final HISN saved as 'hisn_final.parquet'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d15c573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
