{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7227caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 🧠 HYBRID FAKE REVIEW DETECTION PIPELINE (CLEAN VERSION)\n",
    "# Paths: input=./nb2, output=./nb3\n",
    "# ==============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cbeef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 📂 Folder setup\n",
    "# -------------------------------\n",
    "BASE_PATH = Path(\".\")                              # current folder (Text Mining)\n",
    "DATA_PATH = BASE_PATH / \"nb2\"            # input data\n",
    "OUT_PATH  = BASE_PATH / \"nb3\"                      # output folder\n",
    "OUT_PATH.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Input files\n",
    "HISN_PARQUET  = DATA_PATH / \"hisn_final.parquet\"\n",
    "USER_STATS    = DATA_PATH / \"user_stats.csv\"\n",
    "ITEM_STATS    = DATA_PATH / \"item_stats.csv\"\n",
    "REV_PROCESSED = DATA_PATH / \"reviews_processed.parquet\"\n",
    "REV_EMB_NPY   = DATA_PATH / \"review_features.npy\"\n",
    "\n",
    "# Output files\n",
    "OUT_NODES     = OUT_PATH / \"nodes.csv\"\n",
    "OUT_EDGES     = OUT_PATH / \"edges.csv\"\n",
    "OUT_EDGE_EMB  = OUT_PATH / \"edge_emb.npy\"\n",
    "OUT_REPORT    = OUT_PATH / \"hisn_report.json\"\n",
    "OUT_TARGETS   = OUT_PATH / \"targets.csv\"\n",
    "\n",
    "EDGE_SCALARS = [\n",
    "    \"rating_scaled\", \"review_length_scaled\", \"log_helpful_scaled\",\n",
    "    \"verified_int_scaled\", \"dup_count_scaled\",\n",
    "    \"sin_day\", \"cos_day\", \"sin_hour\", \"cos_hour\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dd4fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 1️⃣ EXPORT HISN GRAPH (nodes / edges / embeddings)\n",
    "# ==============================================================\n",
    "\n",
    "def export_hisn_graph():\n",
    "    \"\"\"Build edges, nodes, and aligned embeddings for HISN.\"\"\"\n",
    "    print(\"🚀 Building HISN graph...\")\n",
    "\n",
    "    # --- Load base tables\n",
    "    df_h = pd.read_parquet(HISN_PARQUET)\n",
    "    df_rp = pd.read_parquet(REV_PROCESSED) if REV_PROCESSED.exists() else None\n",
    "    emb = np.load(str(REV_EMB_NPY), mmap_mode=\"r\") if REV_EMB_NPY.exists() else None\n",
    "\n",
    "    # --- Build stable key for alignment\n",
    "    def normalize_ts(s):\n",
    "        ts = pd.to_datetime(s, errors=\"coerce\", utc=True)\n",
    "        return ts.view(\"int64\") // 1_000_000_000\n",
    "\n",
    "    for df in [df_h] + ([df_rp] if df_rp is not None else []):\n",
    "        df[\"_ts\"] = normalize_ts(df.get(\"timestamp\", pd.Timestamp.now()))\n",
    "        df.sort_values([\"user_id\", \"asin\", \"_ts\"], inplace=True)\n",
    "        df[\"_dup\"] = df.groupby([\"user_id\", \"asin\", \"_ts\"]).cumcount()\n",
    "        df[\"rid_key\"] = (\n",
    "            df[\"user_id\"].astype(str) + \"|\" +\n",
    "            df[\"asin\"].astype(str) + \"|\" +\n",
    "            df[\"_ts\"].astype(str) + \"|\" +\n",
    "            df[\"_dup\"].astype(str)\n",
    "        )\n",
    "\n",
    "    # --- Align embeddings\n",
    "    emb_aligned = None\n",
    "    if emb is not None and df_rp is not None:\n",
    "        pos = pd.Series(np.arange(len(df_rp)), index=df_rp[\"rid_key\"])\n",
    "        idx = pos.reindex(df_h[\"rid_key\"])\n",
    "        emb_aligned = np.zeros((len(df_h), emb.shape[1]), dtype=np.float32)\n",
    "        mask = ~idx.isna()\n",
    "        emb_aligned[mask.to_numpy()] = emb[idx[mask].astype(int).to_numpy()]\n",
    "        emb_aligned = np.nan_to_num(emb_aligned, nan=0.0)\n",
    "\n",
    "    # --- Edges\n",
    "    edges = df_h.rename(columns={\"user_id\": \"src\", \"asin\": \"dst\"}).copy()\n",
    "    edges[\"edge_type\"] = \"user-reviews-product\"\n",
    "    edge_keep = [\"src\", \"dst\", \"edge_type\"] + [c for c in EDGE_SCALARS if c in edges]\n",
    "    edges_out = edges[edge_keep]\n",
    "    edges_out.to_csv(str(OUT_EDGES), index=False)\n",
    "\n",
    "    # --- Nodes\n",
    "    users = pd.DataFrame({\"node_id\": df_h[\"user_id\"].astype(str).unique(), \"node_type\": \"user\"})\n",
    "    items = pd.DataFrame({\"node_id\": df_h[\"asin\"].astype(str).unique(), \"node_type\": \"product\"})\n",
    "    nodes = pd.concat([users, items], ignore_index=True)\n",
    "\n",
    "    # --- Merge stats\n",
    "    if USER_STATS.exists():\n",
    "        us = pd.read_csv(USER_STATS).rename(columns={\"user_id\": \"node_id\"})\n",
    "        us[\"node_type\"] = \"user\"\n",
    "        us = us.rename(columns={c: f\"user_{c}\" for c in us.columns if c not in [\"node_id\", \"node_type\"]})\n",
    "        nodes = nodes.merge(us, on=[\"node_id\", \"node_type\"], how=\"left\")\n",
    "\n",
    "    if ITEM_STATS.exists():\n",
    "        it = pd.read_csv(ITEM_STATS).rename(columns={\"asin\": \"node_id\"})\n",
    "        it[\"node_type\"] = \"product\"\n",
    "        it = it.rename(columns={c: f\"item_{c}\" for c in it.columns if c not in [\"node_id\", \"node_type\"]})\n",
    "        nodes = nodes.merge(it, on=[\"node_id\", \"node_type\"], how=\"left\")\n",
    "\n",
    "    nodes.to_csv(str(OUT_NODES), index=False)\n",
    "\n",
    "    # --- Save embeddings\n",
    "    if emb_aligned is not None:\n",
    "        np.save(str(OUT_EDGE_EMB), emb_aligned.astype(np.float32))\n",
    "        print(f\"✅ Exported HISN graph → {OUT_NODES.name}, {OUT_EDGES.name}, {OUT_EDGE_EMB.name}\")\n",
    "    else:\n",
    "        print(\"⚠️ No embeddings found — skipped edge_emb.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8e8c0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 2️⃣ STRUCTURE CHECK / HISN REPORT\n",
    "# ==============================================================\n",
    "\n",
    "def build_graph(nodes, edges):\n",
    "    G = nx.MultiDiGraph()\n",
    "    for _, r in nodes.iterrows():\n",
    "        G.add_node(r[\"node_id\"], **r.to_dict())\n",
    "    for i, r in edges.iterrows():\n",
    "        G.add_edge(r[\"src\"], r[\"dst\"], key=i, **r.to_dict())\n",
    "    return G\n",
    "\n",
    "def run_structure_report():\n",
    "    \"\"\"Basic structure summary for sanity checking.\"\"\"\n",
    "    print(\"🧩 Checking HISN structure...\")\n",
    "    nodes = pd.read_csv(OUT_NODES)\n",
    "    edges = pd.read_csv(OUT_EDGES)\n",
    "    G = build_graph(nodes, edges)\n",
    "\n",
    "    report = {\n",
    "        \"num_nodes\": int(G.number_of_nodes()),\n",
    "        \"num_edges\": int(G.number_of_edges()),\n",
    "        \"node_types\": nodes[\"node_type\"].value_counts().to_dict(),\n",
    "        \"edge_types\": edges[\"edge_type\"].value_counts().to_dict(),\n",
    "        \"components\": nx.number_connected_components(nx.Graph(G)),\n",
    "    }\n",
    "    json.dump(report, open(OUT_REPORT, \"w\"), indent=2)\n",
    "    print(f\"✅ HISN report written → {OUT_REPORT.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90ab9278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 3️⃣ NFS SCORING (suspicious product scoring)\n",
    "# ==============================================================\n",
    "\n",
    "def zscore(s):\n",
    "    s = s.astype(float)\n",
    "    return (s - s.mean()) / (s.std(ddof=0) + 1e-8)\n",
    "\n",
    "def entropy_from_counts(arr):\n",
    "    arr = arr[arr > 0].astype(float)\n",
    "    p = arr / arr.sum()\n",
    "    return -float(np.sum(p * np.log(p + 1e-12)))\n",
    "\n",
    "def resultant_length(sin_vals, cos_vals):\n",
    "    s, c = sin_vals.mean(), cos_vals.mean()\n",
    "    return np.sqrt(s*s + c*c)\n",
    "\n",
    "def compute_nfs():\n",
    "    \"\"\"Compute product-level suspiciousness scores.\"\"\"\n",
    "    print(\"📊 Computing NFS (suspiciousness) scores...\")\n",
    "    df = pd.read_csv(OUT_EDGES)\n",
    "    ucol, pcol = \"src\", \"dst\"\n",
    "\n",
    "    user_deg = df.groupby(ucol).size().rename(\"user_outdeg\")\n",
    "    gb_prod = df.groupby(pcol)\n",
    "\n",
    "    # product stats\n",
    "    prod_deg = gb_prod.size().rename(\"prod_num_reviews\")\n",
    "    unique_users = gb_prod[ucol].nunique().rename(\"prod_unique_users\")\n",
    "\n",
    "    def _entropy(g):\n",
    "        users = g[ucol].values\n",
    "        degs = user_deg.reindex(users).fillna(1).values\n",
    "        return entropy_from_counts(degs)\n",
    "\n",
    "    ent = gb_prod.apply(_entropy).rename(\"reviewer_activity_entropy\")\n",
    "    burst = gb_prod.apply(lambda g: resultant_length(g[\"sin_day\"], g[\"cos_day\"])).rename(\"burst_day\")\n",
    "\n",
    "    dup = gb_prod[\"dup_count_scaled\"].mean().rename(\"dup_mean\") if \"dup_count_scaled\" in df else None\n",
    "    ver = gb_prod[\"verified_int_scaled\"].mean().rename(\"verified_mean\") if \"verified_int_scaled\" in df else None\n",
    "    rating = gb_prod[\"rating_scaled\"].apply(lambda s: s.abs().mean()).rename(\"rating_abs_mean\") if \"rating_scaled\" in df else None\n",
    "\n",
    "    feats = [prod_deg, unique_users, ent, burst]\n",
    "    for opt in [dup, ver, rating]:\n",
    "        if opt is not None:\n",
    "            feats.append(opt)\n",
    "\n",
    "    prod_df = pd.concat(feats, axis=1).fillna(0).reset_index().rename(columns={pcol: \"product_id\"})\n",
    "\n",
    "    # z-scores\n",
    "    z = {\n",
    "        \"z_deg\": zscore(prod_df[\"prod_num_reviews\"]),\n",
    "        \"z_entropy\": -zscore(prod_df[\"reviewer_activity_entropy\"]),\n",
    "        \"z_burst\": zscore(prod_df[\"burst_day\"]),\n",
    "        \"z_dup\": zscore(prod_df[\"dup_mean\"]) if \"dup_mean\" in prod_df else 0,\n",
    "        \"z_ver\": -zscore(prod_df[\"verified_mean\"]) if \"verified_mean\" in prod_df else 0,\n",
    "        \"z_rating\": zscore(prod_df[\"rating_abs_mean\"]) if \"rating_abs_mean\" in prod_df else 0,\n",
    "    }\n",
    "\n",
    "    zdf = pd.DataFrame(z)\n",
    "    nfs = (\n",
    "        1.0*zdf[\"z_deg\"] +\n",
    "        1.0*zdf[\"z_entropy\"] +\n",
    "        1.0*zdf[\"z_burst\"] +\n",
    "        1.0*zdf[\"z_dup\"] +\n",
    "        0.5*zdf[\"z_ver\"] +\n",
    "        0.5*zdf[\"z_rating\"]\n",
    "    )\n",
    "    prod_df[\"NFS_post\"] = nfs\n",
    "    prod_df.sort_values(\"NFS_post\", ascending=False, inplace=True)\n",
    "    prod_df.to_csv(OUT_TARGETS, index=False)\n",
    "    print(f\"✅ Saved NFS scores → {OUT_TARGETS.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba255d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 4️⃣ BUILD HISN BATCHES PER TARGET PRODUCT\n",
    "# ==============================================================\n",
    "\n",
    "def build_batches():\n",
    "    print(\"🧩 Building per-target HISNs...\")\n",
    "    nodes = pd.read_csv(OUT_NODES)\n",
    "    edges = pd.read_csv(OUT_EDGES)\n",
    "    targets = pd.read_csv(OUT_TARGETS)\n",
    "\n",
    "    out_root = OUT_PATH / \"hisn_batch\"\n",
    "    out_root.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for tgt in tqdm(targets[\"product_id\"].astype(str).tolist(), desc=\"Targets\"):\n",
    "        e1 = edges[edges[\"dst\"].astype(str) == tgt]\n",
    "        if e1.empty:\n",
    "            continue\n",
    "        users = e1[\"src\"].astype(str).unique()\n",
    "        e2 = edges[edges[\"src\"].astype(str).isin(users)]\n",
    "        sub_edges = pd.concat([e1, e2]).drop_duplicates(subset=[\"src\",\"dst\"])\n",
    "        node_ids = pd.unique(pd.concat([sub_edges[\"src\"], sub_edges[\"dst\"]]))\n",
    "        sub_nodes = nodes[nodes[\"node_id\"].isin(node_ids)]\n",
    "\n",
    "        out_dir = out_root / tgt\n",
    "        out_dir.mkdir(exist_ok=True, parents=True)\n",
    "        sub_nodes.to_csv(out_dir / \"nodes.csv\", index=False)\n",
    "        sub_edges.to_csv(out_dir / \"edges.csv\", index=False)\n",
    "\n",
    "    print(f\"✅ HISN batches saved → {out_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "296df271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export HISN graph\n",
      "🚀 Building HISN graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_32484\\3827013605.py:17: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  return ts.view(\"int64\") // 1_000_000_000\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_32484\\3827013605.py:17: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  return ts.view(\"int64\") // 1_000_000_000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported HISN graph → nodes.csv, edges.csv, edge_emb.npy\n",
      "Run structure report\n",
      "🧩 Checking HISN structure...\n",
      "✅ HISN report written → hisn_report.json\n",
      "Computing NFS scores\n",
      "📊 Computing NFS (suspiciousness) scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_32484\\2464035872.py:36: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ent = gb_prod.apply(_entropy).rename(\"reviewer_activity_entropy\")\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_32484\\2464035872.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  burst = gb_prod.apply(lambda g: resultant_length(g[\"sin_day\"], g[\"cos_day\"])).rename(\"burst_day\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved NFS scores → targets.csv\n",
      "Building HISN batches\n",
      "🧩 Building per-target HISNs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89395e6a7b4647ae947848b80fc86a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Targets:   0%|          | 0/15881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HISN batches saved → nb3\\hisn_batch\n",
      "🎉 All pipeline stages complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Export HISN graph\")\n",
    "export_hisn_graph()\n",
    "print(\"Run structure report\")\n",
    "run_structure_report()\n",
    "print(\"Computing NFS scores\")\n",
    "compute_nfs()\n",
    "print(\"Building HISN batches\")\n",
    "build_batches()\n",
    "print(\"🎉 All pipeline stages complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc115b47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
